---
title: Second Me - 训练一个自己的 AI 化身 (2503.08102)
date: 2025-03-20T15:32:31.724Z
---

自从 RAG 刚出来的时候，就有不少通过试图往 LLM 里塞自己相关的上下文（笔记/日记），试图让 LLM 更接近自己的尝试了。但是受限于模型能力，以及上下文窗口的限制，总归还是不太接近。另一个方向是微调，但是微调需要大量的 QA 数据，一般人也没有这个心思去问自己各种问题（除非你是传记主角），因此也不算很好推进。

本文提出了一种分层的记忆框架：
- 输入依然是用户自身的原始数据（L0）
- 但是在此之上基于 COT 构造问答对，得到用自然语言描述的用户特征（L1）；
- 最后用这些问答对作为数据源，利用已经被广泛验证过的新训练技术（GRPO） ，微调（LoRA）得到一个存储了用户特性的模型（L2）

后续交互的时候，用 L2 模型作为核心，但是依然用 RAG 从 L0 和 L1 里召回相关信息。虽然没有细读论文，但是感觉上比传统的纯 RAG 和纯微调都更给力一些。

有人可能会问，为什么要训练一个自己的 AI 化身呢？原因其实有很多，但最吸引我的可能是，终于有机会把一些自己时常会思考，但是没有人可以讨论的东西，和“另一个自己”交流。此外在一些重大决策的时候，可能能获得一些更“客观”的视角，让自己的冲动也有迹可循（至少能问 AI 为什么“我”在某个情境下会“这样”做）。

src: https://arxiv.org/abs/2503.08102

related: https://mp.weixin.qq.com/s/3rudrP4IsIu5ejQE_VMbrA