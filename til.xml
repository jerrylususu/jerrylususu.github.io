<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>TIL - Nekonull&#39;s Garden</title>
    <link>https://nekonull.me/</link>
    <description>Recent TIL (Today I Learned) content on Nekonull&#39;s Garden</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <copyright>CC-BY-SA-4.0</copyright>
    <lastBuildDate>Sun, 30 Mar 2025 13:07:41 +0000</lastBuildDate><atom:link href="https://nekonull.me/til.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>coredumpy - 实现事后调试的 Python 库</title>
      <link>https://nekonull.me/til/coredumpy-post-mortem-debugging/</link>
      <pubDate>Sun, 30 Mar 2025 13:07:41 +0000</pubDate>
      
      <guid>https://nekonull.me/til/coredumpy-post-mortem-debugging/</guid>
      <description>&lt;p&gt;Coredump 已经是一个被广为人知的概念了，想法是在出错的时候把当前的完整状态保存下来以供事后分析（Post-mortem debugging）。&lt;code&gt;coredumpy&lt;/code&gt; 为 Python 实现了这一功能，出现异常的时候生成一个 dump 文件，后续可以用 pdb 或者是 VSCode 的调试工具加载，还原事故现场。这个库有一个优势是没有使用 &lt;code&gt;pickle&lt;/code&gt;，因此不要求故障环境和调试环境完全一致，也避免了不安全代码的意外执行。在一个示例中，作者用它调试了一个 CI 问题，看起来比传统排查效率高非常多。&lt;/p&gt;
&lt;p&gt;src: &lt;a href=&#34;https://github.com/gaogaotiantian/coredumpy&#34;&gt;https://github.com/gaogaotiantian/coredumpy&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;video: &lt;a href=&#34;https://www.bilibili.com/video/BV1nRQkYoE2c&#34;&gt;https://www.bilibili.com/video/BV1nRQkYoE2c&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Node Module Inspector</title>
      <link>https://nekonull.me/til/node-module-inspector/</link>
      <pubDate>Fri, 28 Mar 2025 15:55:59 +0000</pubDate>
      
      <guid>https://nekonull.me/til/node-module-inspector/</guid>
      <description>&lt;p&gt;可视化展示本地 node_modules 文件夹中的依赖项的小工具，包括依赖关系、大小分布等。&lt;/p&gt;
&lt;p&gt;src: &lt;a href=&#34;https://github.com/antfu/node-modules-inspector&#34;&gt;https://github.com/antfu/node-modules-inspector&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;example: &lt;a href=&#34;https://everything.antfu.dev/chart&#34;&gt;https://everything.antfu.dev/chart&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>让 LLM 从文件生成美观的网页的 prompt</title>
      <link>https://nekonull.me/til/prompt-for-visualize-file-in-style/</link>
      <pubDate>Thu, 27 Mar 2025 14:16:48 +0000</pubDate>
      
      <guid>https://nekonull.me/til/prompt-for-visualize-file-in-style/</guid>
      <description>&lt;p&gt;之前看到过这种低饱和度的美观网页，不过从这个 prompt 才知道原来可以这个组件库叫做 &lt;a href=&#34;https://preline.co/&#34;&gt;Preline UI&lt;/a&gt;。仅仅是声明特定组件库，就可以对输出效果有巨大影响。Deepseek V3 最近的更新应该也起到了很大作用。&lt;/p&gt;
&lt;p&gt;用手头的不同模型试了下，Claude 3.7 / Deepseek V3 / Genimi 2.5 Pro 基本都在一个水平线上。用 Cursor/Windsurf 的话需要注意单次连续输出可能超出长度限制；网页对话的话可能会因为每次输出都会重跑语法高亮导致页面卡死，建议生成过程中切换到其他标签页。&lt;/p&gt;
&lt;p&gt;虽然效果很好，但是目前用 Tailwind 会导致模型消耗大量 token 在各种类名上。可能如果有更语义化的框架会更有效率一些？&lt;/p&gt;
&lt;p&gt;具体 prompt 如下，来源附后：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;我会给你一个文件，分析内容，并将其转化为美观漂亮的中文可视化网页：
## 内容要求
- 所有页面内容必须为简体中文
- 保持原文件的核心信息，但以更易读、可视化的方式呈现
- 在页面底部添加作者信息区域，包含：    
  - 作者姓名: []    
  - 社交媒体链接: 至少包含Twitter/X
  - 版权信息和年份

## 设计风格
- 整体风格参考Linear App的简约现代设计
- 使用清晰的视觉层次结构，突出重要内容
- 配色方案应专业、和谐，适合长时间阅读

## 技术规范
- 使用HTML5、TailwindCSS 3.0+（通过CDN引入）和必要的JavaScript
- **使用CDN引入Preline UI组件库，按需使用其组件增强界面效果**
- **根据提供的JSON文件内容（颜色、字体等）配置TailwindCSS的样式Token，确保设计一致性**
- 实现完整的深色/浅色模式切换功能，默认跟随系统设置
- 代码结构清晰，包含适当注释，便于理解和维护

## 响应式设计
- 页面必须在所有设备上（手机、平板、桌面）完美展示
- 针对不同屏幕尺寸优化布局和字体大小
- 确保移动端有良好的触控体验

## 媒体资源
- 使用文档中的Markdown图片链接（如果有的话）
- 使用文档中的视频嵌入代码（如果有的话）

## 图标与视觉元素
- 使用专业图标库如Font Awesome或Material Icons（通过CDN引入）
- 根据内容主题选择合适的插图或图表展示数据
- 避免使用emoji作为主要图标

## 交互体验
- 添加适当的微交互效果提升用户体验：    
  - 按钮悬停时有轻微放大和颜色变化    
  - 卡片元素悬停时有精致的阴影和边框效果    
  - 页面滚动时有平滑过渡效果    
  - 内容区块加载时有优雅的淡入动画

## 性能优化
- 确保页面加载速度快，避免不必要的大型资源
- 图片使用现代格式(WebP)并进行适当压缩
- 实现懒加载技术用于长页面内容

## 输出要求
- 提供完整可运行的单一HTML文件，包含所有必要的CSS和JavaScript
- 确保代码符合W3C标准，无错误警告
- 页面在不同浏览器中保持一致的外观和功能
请根据上传文件的内容类型（文档、数据、图片等），创建最适合展示该内容的可视化网页。


参考导入
    &amp;lt;script src=&amp;#34;https://cdn.tailwindcss.com&amp;#34;&amp;gt;&amp;lt;/script&amp;gt;
    &amp;lt;link rel=&amp;#34;stylesheet&amp;#34; href=&amp;#34;https://cdn.jsdelivr.net/npm/preline@1.8.0/dist/preline.min.css&amp;#34;&amp;gt;
    &amp;lt;link rel=&amp;#34;stylesheet&amp;#34; href=&amp;#34;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css&amp;#34;&amp;gt;
    &amp;lt;script src=&amp;#34;https://cdn.jsdelivr.net/npm/preline@1.8.0/dist/preline.min.js&amp;#34;&amp;gt;&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;src: &lt;a href=&#34;https://mp.weixin.qq.com/s/fxzb8m-THjyzdOlXPuCFnQ&#34;&gt;https://mp.weixin.qq.com/s/fxzb8m-THjyzdOlXPuCFnQ&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Think Tool - 用 Noop Tool Call 让 LLM 记录自己的思考</title>
      <link>https://nekonull.me/til/think-tool-no-op-tool-call/</link>
      <pubDate>Sat, 22 Mar 2025 12:28:12 +0000</pubDate>
      
      <guid>https://nekonull.me/til/think-tool-no-op-tool-call/</guid>
      <description>&lt;p&gt;给可用工具列表新增一个 &lt;code&gt;think&lt;/code&gt; 工具，只有一个必填参数 &lt;code&gt;thought&lt;/code&gt;；这个工具什么都不做，唯一的作用就是让 thought 可以在后续对话中被 tool_call 块携带。&lt;/p&gt;
&lt;p&gt;因为是标准的 tool call，所以对于非推理模型也能用。看起来似乎在 prompt 对应调优过的情况下表现最佳，但是单独用似乎也不错。值得一试。&lt;/p&gt;
&lt;p&gt;src: &lt;a href=&#34;https://www.anthropic.com/engineering/claude-think-tool&#34;&gt;https://www.anthropic.com/engineering/claude-think-tool&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;via: &lt;a href=&#34;https://simonwillison.net/2025/Mar/21/the-think-tool/#atom-everything&#34;&gt;https://simonwillison.net/2025/Mar/21/the-think-tool/#atom-everything&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>TypeScript 中的错误处理光谱</title>
      <link>https://nekonull.me/til/typescript-error-handling-spectrum/</link>
      <pubDate>Thu, 20 Mar 2025 15:56:18 +0000</pubDate>
      
      <guid>https://nekonull.me/til/typescript-error-handling-spectrum/</guid>
      <description>&lt;p&gt;看了 Theo 的一个视频，讲到了 TypeScript 中的错误处理，把各种“流派”按照对错误的态度（从不在意到视为核心）划分了一个光谱：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;完全不管：默认状态，到处 try catch （很不幸我也是这样的）&lt;/li&gt;
&lt;li&gt;低度关心：认识到错误需要被明确表达，而不是被隐藏在执行流中；用 tryCatch 或类似实现，让错误在调用返回中明确（&lt;code&gt;const {data, err} = await someFunc()&lt;/code&gt;，类似于 Go 的写法）&lt;/li&gt;
&lt;li&gt;中等关心：认为需要一个专门的库来封装错误；例如 &lt;code&gt;neverthrow&lt;/code&gt;，实现了 Rust 风格的 Ok, Err 和 chaining&lt;/li&gt;
&lt;li&gt;高度关心：认为需要围绕错误和异常状态设计整个应用；例如 effect 作为一个完整的框架，写起来其实已经不像是 TypeScript 了&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;我自己在写代码的时候，其实有的时候也会对到处 try-catch，catch 了又 throw 有点烦躁，但是不知道什么是更好的做法。看起来可以试试低度关心，再给错误层级有合理封装（至少 throw 之前包一层）。&lt;/p&gt;
&lt;p&gt;video: &lt;a href=&#34;https://www.youtube.com/watch?v=Y6jT-IkV0VM&#34;&gt;https://www.youtube.com/watch?v=Y6jT-IkV0VM&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;tryCatch: &lt;a href=&#34;https://gist.github.com/t3dotgg/a486c4ae66d32bf17c09c73609dacc5b&#34;&gt;https://gist.github.com/t3dotgg/a486c4ae66d32bf17c09c73609dacc5b&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;neverthrow: &lt;a href=&#34;https://github.com/supermacro/neverthrow&#34;&gt;https://github.com/supermacro/neverthrow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;effect: &lt;a href=&#34;https://effect.website/&#34;&gt;https://effect.website/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Block Diffusion - 分块扩散的半自回归模型 (2503.09573)</title>
      <link>https://nekonull.me/til/block-diffusion-half-autoregressive-llm/</link>
      <pubDate>Thu, 20 Mar 2025 15:47:10 +0000</pubDate>
      
      <guid>https://nekonull.me/til/block-diffusion-half-autoregressive-llm/</guid>
      <description>&lt;p&gt;又是一个 “middle ground” 的好主意！&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;自回归模型：效果好，输出长度范围大，但是没法并行&lt;/li&gt;
&lt;li&gt;扩散模型：效果差，输出长度固定，但是很好并行&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;分块扩散模型：输出分成多个块，块间自回归，块内扩散；既有自回归的效果（虽然没有纯自回归那么好），又有扩散模型的高并行度（虽然也没有纯扩散模型那么高）。&lt;/p&gt;
&lt;p&gt;不过其实我对扩散模型有好感，其实是因为去噪过程中 token 是可以变化的，意味着模型“原生”就可以改主意。这不像自回归模型一旦 token 输出了就 commit 不能再改了；即使有 reasoning token 这种突破，但是本质上还是存在输出的不可改变性。&lt;/p&gt;
&lt;p&gt;src: &lt;a href=&#34;https://arxiv.org/abs/2503.09573&#34;&gt;https://arxiv.org/abs/2503.09573&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Second Me - 训练一个自己的 AI 化身 (2503.08102)</title>
      <link>https://nekonull.me/til/second-me-self-as-ai-model/</link>
      <pubDate>Thu, 20 Mar 2025 15:32:31 +0000</pubDate>
      
      <guid>https://nekonull.me/til/second-me-self-as-ai-model/</guid>
      <description>&lt;p&gt;自从 RAG 刚出来的时候，就有不少通过试图往 LLM 里塞自己相关的上下文（笔记/日记），试图让 LLM 更接近自己的尝试了。但是受限于模型能力，以及上下文窗口的限制，总归还是不太接近。另一个方向是微调，但是微调需要大量的 QA 数据，一般人也没有这个心思去问自己各种问题（除非你是传记主角），因此也不算很好推进。&lt;/p&gt;
&lt;p&gt;本文提出了一种分层的记忆框架：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;输入依然是用户自身的原始数据（L0）&lt;/li&gt;
&lt;li&gt;但是在此之上基于 COT 构造问答对，得到用自然语言描述的用户特征（L1）；&lt;/li&gt;
&lt;li&gt;最后用这些问答对作为数据源，利用已经被广泛验证过的新训练技术（GRPO） ，微调（LoRA）得到一个存储了用户特性的模型（L2）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;后续交互的时候，用 L2 模型作为核心，但是依然用 RAG 从 L0 和 L1 里召回相关信息。虽然没有细读论文，但是感觉上比传统的纯 RAG 和纯微调都更给力一些。&lt;/p&gt;
&lt;p&gt;有人可能会问，为什么要训练一个自己的 AI 化身呢？原因其实有很多，但最吸引我的可能是，终于有机会把一些自己时常会思考，但是没有人可以讨论的东西，和“另一个自己”交流。此外在一些重大决策的时候，可能能获得一些更“客观”的视角，让自己的冲动也有迹可循（至少能问 AI 为什么“我”在某个情境下会“这样”做）。&lt;/p&gt;
&lt;p&gt;src: &lt;a href=&#34;https://arxiv.org/abs/2503.08102&#34;&gt;https://arxiv.org/abs/2503.08102&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;related: &lt;a href=&#34;https://mp.weixin.qq.com/s/3rudrP4IsIu5ejQE_VMbrA&#34;&gt;https://mp.weixin.qq.com/s/3rudrP4IsIu5ejQE_VMbrA&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Wasp - 用于构建全栈网站的元框架</title>
      <link>https://nekonull.me/til/wasp-as-a-meta-framework/</link>
      <pubDate>Tue, 18 Mar 2025 15:54:33 +0000</pubDate>
      
      <guid>https://nekonull.me/til/wasp-as-a-meta-framework/</guid>
      <description>&lt;p&gt;一个看起来比较有趣的项目。构建网站的时候通常有很多重复工作（登录/鉴权/邮件&amp;hellip;）。为了减少重复，作者提出了一种元框架，在现有的前后端框架之上再叠加一层，写一些 DSL 来定义一个“网站”，然后再通过一个自定义的“编译器”生成底层的框架结构。&lt;/p&gt;
&lt;p&gt;虽然感觉能解决一些常见问题，但是如果不能满足需求的话，自定义起来可能会是灾难。&lt;/p&gt;
&lt;p&gt;DSL 示例如下：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;app todoApp {
  title: &amp;#34;ToDo App&amp;#34;,  // visible in the browser tab
  auth: { // full-stack auth out-of-the-box
    userEntity: User, 
    methods: { google: {}, gitHub: {}, email: {...} }
  }
}

route RootRoute { path: &amp;#34;/&amp;#34;, to: MainPage }
page MainPage {
  authRequired: true, // Limit access to logged in users.
  component: import Main from &amp;#34;@client/Main.tsx&amp;#34; // Your React code.
}

query getTasks {
  fn: import { getTasks } from &amp;#34;@server/tasks.js&amp;#34;, // Your Node.js code.
  entities: [Task] // Automatic cache invalidation.
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;src: &lt;a href=&#34;https://wasp.sh/&#34;&gt;https://wasp.sh/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>DVC - Data Version Control</title>
      <link>https://nekonull.me/til/data-version-control/</link>
      <pubDate>Tue, 18 Mar 2025 15:38:44 +0000</pubDate>
      
      <guid>https://nekonull.me/til/data-version-control/</guid>
      <description>&lt;p&gt;看起来像是一个给数据集用的 Git，还整合了一些模型训练相关的记录和工作流能力（例如对比不同实验，以及把输入、实验和输出结合起来）。&lt;/p&gt;
&lt;p&gt;src: &lt;a href=&#34;https://dvc.org/&#34;&gt;https://dvc.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;via: &lt;a href=&#34;https://github.com/dfsnow/opentimes&#34;&gt;https://github.com/dfsnow/opentimes&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>对代码的上下文检索（Contextual Retrieval）</title>
      <link>https://nekonull.me/til/code-contextual-retrieval/</link>
      <pubDate>Mon, 17 Mar 2025 15:07:18 +0000</pubDate>
      
      <guid>https://nekonull.me/til/code-contextual-retrieval/</guid>
      <description>&lt;p&gt;在用各类 LLM 编程工具的时候，难免会遇到“进一步退两步”的情况，例如改一个功能但是把不相关的代码改坏了，或者是为一个简单的功能引入了完全不必要的抽象层。造成这种现象的原因之一，是 LLM 对于项目的上下文并没有明确的感知（不知道应该改哪里、怎么改、会影响什么）。虽然可能在开干之前会读一些代码，但是考虑到每次对话都是全新的，不难想象这样获取到的上下文实际上是不完备的。&lt;/p&gt;
&lt;p&gt;今天在阅读他人的博客时，了解到了一种让模型自动总结现有代码库，主动生成一些摘要的做法。这样可以让模型在遇到问题时参考已经生成过的摘要，从而获得对项目结构和惯用法的了解，让模型的输出结果更有用（或者说更接地气 Grounding？）。摘要大概类似下面这样：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;1. `Authentication Flow`
   - Importance Score: 99
   - Implementation:
     - User signs up with email/password or OAuth provider
     - Verification email sent with secure token
     - On verification, JWT access and refresh tokens issued
     - Access token used for API requests (1 hour expiry)
     - Refresh token used to obtain new access tokens (7 day expiry)
   - Key Files: 
     - `apps/api/services/authService.ts`
     - `apps/web/hooks/useAuth.ts`
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;突然想到，从代码库里找到一个文件，并将其和代码库里的其他文件联系起来（构建上下文）的流程，其实很类似 RAG 流程里从无数个文档块（chunk）里找到一块（例如用向量搜索），然后将其和文档联系起来的流程。延申想想，去年 Anthropic 出过一个叫做 Contextual Retrieval （上下文检索）的工作，利用前缀缓存，给每个 chunk 用 {full_text} + {chunk} 生成一个 context，再把这个 context 和 chunk 一起存储和检索。这一想法，是否也能应用于代码领域呢？&lt;/p&gt;
&lt;p&gt;假设有一个上下文窗口足够大，且开启了前缀缓存的模型，一个可能的示例如下：&lt;/p&gt;
&lt;p&gt;构建阶段：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;把代码库所有文件拼接为一个巨大的 codebase_str&lt;/li&gt;
&lt;li&gt;对每个代码文件 file
&lt;ul&gt;
&lt;li&gt;用 system_prompt + codebase_str + file 调用 LLM，得到 context（在这个系统里，这段代码的作用是&amp;hellip;，和其他代码的联系是&amp;hellip;）&lt;/li&gt;
&lt;li&gt;把 context 和 code 关联存储（可能是一个 SQLite 数据库，也可能是一个简单的 JSON，原始 code 文件的 path 作为 key）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;检索和生成阶段&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;根据用户的指示，同时在 code 和 context 上做召回&lt;/li&gt;
&lt;li&gt;调用 LLM 时，对于每个 code 文件，同时提供其在整个项目的 context&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;听起来似乎有点希望。&lt;/p&gt;
&lt;p&gt;src: &lt;a href=&#34;https://nmn.gl/blog/cursor-guide&#34;&gt;https://nmn.gl/blog/cursor-guide&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;link: &lt;a href=&#34;https://gigamind.dev/&#34;&gt;https://gigamind.dev/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;releated: &lt;a href=&#34;https://www.anthropic.com/news/contextual-retrieval&#34;&gt;https://www.anthropic.com/news/contextual-retrieval&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>LLM 是现实世界的快照</title>
      <link>https://nekonull.me/til/llm-as-world-snapshot/</link>
      <pubDate>Mon, 17 Mar 2025 14:22:19 +0000</pubDate>
      
      <guid>https://nekonull.me/til/llm-as-world-snapshot/</guid>
      <description>&lt;p&gt;读到了 Redis 作者 antirez 的一篇文章《LLM 的权重是历史的一部分》。&lt;/p&gt;
&lt;p&gt;的确，网络上越来越多的信息已经永久消失了（现在能打开一个 2010~2020 年之间的某个链接都已经几乎是奇迹了）。存档的工作很重要，但是这样做并没有经济效益。已训练的 LLM 虽然可能会有幻觉，但至少代表了某一个时刻的现实。而且考虑到模型分散的足够多，可能比传统的集中式存档（例如 Internet Archive）更能持久的保存下来当前我们所见的世界。&lt;/p&gt;
&lt;p&gt;src: &lt;a href=&#34;https://antirez.com/news/147&#34;&gt;https://antirez.com/news/147&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>浏览器内置的默认 HTML 样式</title>
      <link>https://nekonull.me/til/browser-default-css/</link>
      <pubDate>Mon, 17 Mar 2025 14:07:45 +0000</pubDate>
      
      <guid>https://nekonull.me/til/browser-default-css/</guid>
      <description>&lt;p&gt;HTML 如果没有指定一个元素的样式，那么会用“默认”样式展示。（在 DevTools 里似乎被标记为“用户代理样式表”）。之前一直有些疑惑到底是在哪里定义的，今天在读其他信息的时候发现了。&lt;/p&gt;
&lt;p&gt;src: &lt;a href=&#34;https://github.com/chromium/chromium/blob/main/third_party/blink/renderer/core/html/resources/html.css&#34;&gt;https://github.com/chromium/chromium/blob/main/third_party/blink/renderer/core/html/resources/html.css&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;via: &lt;a href=&#34;https://simonwillison.net/2025/Mar/16/backstory/&#34;&gt;https://simonwillison.net/2025/Mar/16/backstory/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Claude 的 Text Editor Tool</title>
      <link>https://nekonull.me/til/claude-text-editor-tool/</link>
      <pubDate>Fri, 14 Mar 2025 13:45:33 +0000</pubDate>
      
      <guid>https://nekonull.me/til/claude-text-editor-tool/</guid>
      <description>&lt;p&gt;Claude 最近新支持了一个特殊的工具：Text Editor Tool，本质上是一组预定义的接口，让 LLM 可以实现文本编辑（不过看起来更像是为了代码编辑准备的）。具体定义的接口有 view, create, str_replace, insert, undo_edit。&lt;/p&gt;
&lt;p&gt;文档里一些比较有趣的点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;view 文件的时候，可以指定特定的 range（有助于处理特别大的文件）&lt;/li&gt;
&lt;li&gt;view 的时候，推荐把行号也塞进去（例如&lt;code&gt;1:import os&lt;/code&gt;），这样能有助于编辑（因为 insert 需要指定行号）&lt;/li&gt;
&lt;li&gt;str_replace 的时候应该只有一个精确匹配，匹配不存在或者有多个匹配都是 error&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;一些自己的思考：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对比其他基于 diff 的编辑方式（例如 cline），用行号来声明编辑范围的确会更省 token，但是也对模型能力提出了更高要求（算不对行号的话很容易导致编辑失败）。&lt;/li&gt;
&lt;li&gt;str_replace 只能有一个精确匹配的行为也比较诡异，且参数没有 range，感觉上对于重构似乎不太友好（例如把一个文件里的所有 var1a 换成 slight_better_name）&lt;/li&gt;
&lt;li&gt;只有 create / insert，但没有 update / delete / remove？难道真就只增不减了？&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这个工具具体是否有用，还是等各路开发者的验证了。毕竟官方只定义了接口，具体实现当然是“留作练习”了。&lt;/p&gt;
&lt;p&gt;src: &lt;a href=&#34;https://docs.anthropic.com/en/docs/build-with-claude/tool-use/text-editor-tool&#34;&gt;https://docs.anthropic.com/en/docs/build-with-claude/tool-use/text-editor-tool&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;via: &lt;a href=&#34;https://simonwillison.net/2025/Mar/13/anthropic-api-text-editor-tool/#atom-everything&#34;&gt;https://simonwillison.net/2025/Mar/13/anthropic-api-text-editor-tool/#atom-everything&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>游牧计算</title>
      <link>https://nekonull.me/til/on-nomadic-compute/</link>
      <pubDate>Tue, 11 Mar 2025 14:59:03 +0000</pubDate>
      
      <guid>https://nekonull.me/til/on-nomadic-compute/</guid>
      <description>&lt;p&gt;在使用 LLM 的时候很容易陷入两个极端之一：完全信任服务提供商（无论是 OpenAI/Deepseek 这样的官方服务，还是像 SiliconFlow/OpenRouter 这样的推理提供商），或者是完全自己部署。前者很方便，外包了复杂度，但是也意味着模型不受自己控制；后者很自由，但前提是得有足够多的钱买卡。&lt;/p&gt;
&lt;p&gt;但其实全托管-自托管是一段连续的光谱，游牧计算（Nomadic Compute）则是其中一个有趣的中间点。这个概念似乎一位外国博主 Xe Iaso 创造的，指的是像游牧民族那样，带着自己的数据在不同的云服务基础设施提供商之间漫游，逐水草而居（实际上是价格最低的 GPU）。这样的行为方式之所以能成立，是因为计算（Nvidia GPU）和存储（假设使用某种对象存储服务，例如 S3 或者各种 S3-兼容服务）都是云供应商中立，可以互换的。只要你在需要时在价格最低的云上启动实例，并在不用的时候完全关闭，就能享受到相对低的价格和更自由的选择，并且避免供应商锁定。&lt;/p&gt;
&lt;p&gt;如果这听起来很有趣，你应该继续阅读这篇演讲记录。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://xeiaso.net/talks/2025/ai-chatbot-friends/&#34;&gt;https://xeiaso.net/talks/2025/ai-chatbot-friends/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;或者看看已有的帮你自动跨云服务商找最低价 GPU 并完成计算的开源项目&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/skypilot-org/skypilot/&#34;&gt;https://github.com/skypilot-org/skypilot/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>用 LLM 计算可视化文字的信息增量</title>
      <link>https://nekonull.me/til/llm-perplexity-visualization/</link>
      <pubDate>Sun, 09 Mar 2025 15:39:03 +0000</pubDate>
      
      <guid>https://nekonull.me/til/llm-perplexity-visualization/</guid>
      <description>&lt;p&gt;读 LLM 推理相关的资料时，想到：在 LLM 前向推理的过程中，会计算从输入开头到当前 token 的所有 token 的 logit，最后采样的时候只用了最后一个 token 的 logit；是否可以构建一个工具，用户输入一段文本，让LLM计算每个token上出现用户输入token的概率，并且加点可视化（概率大：说明这个接续很常见，没什么信息增量；概率小：说明这个接续不太常见，可能有点意思），来得到一个展示文本信息增量的工具？&lt;/p&gt;
&lt;p&gt;搜了搜，发现果然已经有人做过了，用的是 GPT-2 （虽然老了一些，但是足够小，可以直接跑在浏览器里），效果也很棒。或许可以考虑下用现代的 SLM（Small Language Model）更新下？&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://perplexity.vercel.app/&#34;&gt;https://perplexity.vercel.app/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Emoji Kitchen - 用 2 个已有的 emoji 生成新的 emoji</title>
      <link>https://nekonull.me/til/emoji-kitchen/</link>
      <pubDate>Sat, 08 Mar 2025 12:41:00 +0000</pubDate>
      
      <guid>https://nekonull.me/til/emoji-kitchen/</guid>
      <description>&lt;p&gt;虽然我是 Gboard 输入法的用户，但是因为日常用 Emoji 不多，直到最近才了解到 Emoji Kitchen 这个功能（或者说项目？）。简而言之，给定两个 Emoji，可以生成一个符合加法语义的新 Emoji （实际上是图像，而不是 Unicode 标准的字符）。例如 热狗+花束=热狗花束。&lt;/p&gt;
&lt;p&gt;阅读相关资料可以发现，每一个组合都是 Google 的设计团队手绘的[1]；虽然有一些组合原则[2]，但是没有自动化（至少从我能找到的信息来看是这样）。考虑到图像生成模型已经广泛可用了，可能未来会有更多的组合是生成而不是手绘的？&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;we&amp;rsquo;ve drawn over 30,000 images in just a few years lmao&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://emojikitchen.dev/&#34;&gt;https://emojikitchen.dev/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;ref1: &lt;a href=&#34;https://jenniferdaniel.substack.com/p/introducing-emoji-kitchen-&#34;&gt;https://jenniferdaniel.substack.com/p/introducing-emoji-kitchen-&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;ref2: &lt;a href=&#34;https://9to5google.com/2024/12/27/gboard-android-emoji-kitchen-list/&#34;&gt;https://9to5google.com/2024/12/27/gboard-android-emoji-kitchen-list/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>用 PATH 里的脚本是更好的别名</title>
      <link>https://nekonull.me/til/scripts-in-path-can-do-more-than-alias/</link>
      <pubDate>Thu, 06 Mar 2025 14:10:10 +0000</pubDate>
      
      <guid>https://nekonull.me/til/scripts-in-path-can-do-more-than-alias/</guid>
      <description>&lt;p&gt;我自己的 .bashrc 里有不少小工具，但是要每次改起来得写 shell script 着实烦人（即使能用 LLM 写，也免不了来回调试几次才能满意）。读到这篇文章，我才意识到原来可以写脚本放到 script 里，还有一些额外的优势（不用手动 source 重载、用 shell script 之外的任何语言、各种 shell 都能用）。&lt;/p&gt;
&lt;p&gt;src: &lt;a href=&#34;https://evanhahn.com/why-alias-is-my-last-resort-for-aliases/&#34;&gt;https://evanhahn.com/why-alias-is-my-last-resort-for-aliases/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Rust 的日期库的奇妙实现</title>
      <link>https://nekonull.me/til/rust-date-function-math-magic/</link>
      <pubDate>Sun, 02 Mar 2025 13:51:04 +0000</pubDate>
      
      <guid>https://nekonull.me/til/rust-date-function-math-magic/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;问题：实现一个函数，输入 (year, day-of-year) ，输出 (year, month, day)&lt;/li&gt;
&lt;li&gt;基本做法：硬编码一个月份-日期数组，简单写一个循环&lt;/li&gt;
&lt;li&gt;Rust 的新做法：利用一些神奇的数学关系（仿射函数），可以在无循环的情况下实现判断，核心是这两个公式：&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;let month = (ordinal + 30) * 10 / 306 + 2;
let days_in_preceding_months = (month + 3) * 306 / 10 - 122;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;最终优化后的代码如下：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-rust&#34; data-lang=&#34;rust&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;fn&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;ordinal_date_to_calendar_date&lt;/span&gt;(year: &lt;span style=&#34;color:#66d9ef&#34;&gt;i32&lt;/span&gt;, ordinal: &lt;span style=&#34;color:#66d9ef&#34;&gt;u16&lt;/span&gt;) -&amp;gt; (&lt;span style=&#34;color:#66d9ef&#34;&gt;i32&lt;/span&gt;, &lt;span style=&#34;color:#66d9ef&#34;&gt;u8&lt;/span&gt;, &lt;span style=&#34;color:#66d9ef&#34;&gt;u8&lt;/span&gt;) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; ordinal &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ordinal &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;u32&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; jan_feb_len &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;59&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; is_leap_year(year) &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;u32&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; (month_adj, ordinal_adj) &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; ordinal &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;=&lt;/span&gt; jan_feb_len {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        (&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    } &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        (&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, jan_feb_len)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    };
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; ordinal &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ordinal &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; ordinal_adj;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; month &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (ordinal &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;268&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;8031&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;13&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; days_in_preceding_months &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (month &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;3917&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;3866&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; day &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ordinal &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; days_in_preceding_months;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; month &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; month &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; month_adj;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    (year, month &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; _, day &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; _)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;src: &lt;a href=&#34;https://jhpratt.dev/blog/optimizing-with-novel-calendrical-algorithms/&#34;&gt;https://jhpratt.dev/blog/optimizing-with-novel-calendrical-algorithms/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>用 Unicode 变体选择器隐藏信息</title>
      <link>https://nekonull.me/til/hide-info-unicode-var-selector/</link>
      <pubDate>Sun, 02 Mar 2025 13:46:14 +0000</pubDate>
      
      <guid>https://nekonull.me/til/hide-info-unicode-var-selector/</guid>
      <description>&lt;p&gt;用 variation selector (VS 变体选择器） 把二进制数据藏在 unicode 字符串里；VS 有 256 个（VS1~VS256），刚好一个 byte 一个，二进制数据甚至不需要特殊处理（例如 base64）&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;一般应用（如浏览器）不会渲染出来&lt;/li&gt;
&lt;li&gt;LLM 可以阅读（很适合做 prompt injecting）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;src: &lt;a href=&#34;https://paulbutler.org/2025/smuggling-arbitrary-data-through-an-emoji/&#34;&gt;https://paulbutler.org/2025/smuggling-arbitrary-data-through-an-emoji/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>LLMSerp - 把大语言模型当作搜索引擎</title>
      <link>https://nekonull.me/til/llmserp-llm-as-search-engine-page/</link>
      <pubDate>Sun, 02 Mar 2025 09:33:03 +0000</pubDate>
      
      <guid>https://nekonull.me/til/llmserp-llm-as-search-engine-page/</guid>
      <description>&lt;p&gt;LLM 内部已经存储了很多事实性信息。如果通过一些 prompt，让 LLM 输出 [{title, abstract, url}]，就能实现一个搜索引擎。通过把这个&amp;quot;假&amp;quot;搜索引擎和真搜索引擎（例如 Google）的结果混合，Jina.AI 发现可以解决 RAG 中“什么时候需要调用外部搜索”的问题（可以阅读他们的博客文章了解更多信息）。但即使没有这个作用，单纯把 LLM 当作搜索引擎用也是一个很神奇的想法。&lt;/p&gt;
&lt;p&gt;demo: &lt;a href=&#34;https://jina.ai/llm-serp-demo/&#34;&gt;https://jina.ai/llm-serp-demo/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;blog: &lt;a href=&#34;https://jina.ai/news/llm-as-serp-search-engine-result-pages-from-large-language-models&#34;&gt;https://jina.ai/news/llm-as-serp-search-engine-result-pages-from-large-language-models&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>debugpy - VSCode 无配置文件调试 Python</title>
      <link>https://nekonull.me/til/debugpy-vscode/</link>
      <pubDate>Sun, 02 Mar 2025 09:01:58 +0000</pubDate>
      
      <guid>https://nekonull.me/til/debugpy-vscode/</guid>
      <description>&lt;p&gt;VSCode 里用 debugpy 可以直接启动 debug session，无需配置文件（launch.json）。基于 &lt;a href=&#34;https://microsoft.github.io/debug-adapter-protocol/&#34;&gt;Debug Adapter Protocol&lt;/a&gt;，和 Language Server Protocl 类似的调试协议。&lt;/p&gt;
&lt;p&gt;src: &lt;a href=&#34;https://github.com/microsoft/vscode-python-debugger/wiki/No%E2%80%90Config-Debugging&#34;&gt;https://github.com/microsoft/vscode-python-debugger/wiki/No%E2%80%90Config-Debugging&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;via: &lt;a href=&#34;https://www.bitecode.dev/p/whats-up-python-better-packaging&#34;&gt;https://www.bitecode.dev/p/whats-up-python-better-packaging&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>要求 LLM 在完成任务时留下笔记</title>
      <link>https://nekonull.me/til/make-llm-take-notes-when-finishing-task/</link>
      <pubDate>Thu, 28 Nov 2024 13:39:17 +0000</pubDate>
      
      <guid>https://nekonull.me/til/make-llm-take-notes-when-finishing-task/</guid>
      <description>&lt;p&gt;ref: &lt;a href=&#34;https://addyosmani.com/blog/automated-decision-logs/&#34;&gt;https://addyosmani.com/blog/automated-decision-logs/&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;在项目目录下放一个 &lt;code&gt;fyi.md&lt;/code&gt; 或者 &lt;code&gt;notes.md&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;在 prompt 里要求 LLM 完成任务时同时更新此笔记，包含目标、方案和实际执行的行动&lt;/li&gt;
&lt;/ol&gt;&lt;/blockquote&gt;
&lt;p&gt;这看起来是一个显而易见的有效做法，但是之前却没有人提出过！下次使用 cursor 或者其他 LLM 加持的编辑器时我肯定会试试这样做。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>curl 有 LTS 了</title>
      <link>https://nekonull.me/til/curl-has-5-year-lts/</link>
      <pubDate>Sun, 17 Nov 2024 13:27:04 +0000</pubDate>
      
      <guid>https://nekonull.me/til/curl-has-5-year-lts/</guid>
      <description>&lt;p&gt;curl 有 5 年 LTS (Long Time Support) 版本了，不过只有签了商业协议的用户才能使用&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://rock-solid.curl.dev/&#34;&gt;https://rock-solid.curl.dev/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;via: &lt;a href=&#34;https://daniel.haxx.se/blog/2024/11/07/rock-solid-curl/&#34;&gt;https://daniel.haxx.se/blog/2024/11/07/rock-solid-curl/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Excel 的自动填充实际上是程序生成</title>
      <link>https://nekonull.me/til/excel-auto-complete-as-program-systhenis/</link>
      <pubDate>Sun, 17 Nov 2024 13:36:00 +0800</pubDate>
      
      <guid>https://nekonull.me/til/excel-auto-complete-as-program-systhenis/</guid>
      <description>&lt;p&gt;原来 Excel 里自动填充是有论文的，底层是实现了程序生成；另外据说 Google Sheets 的自动填充之所以没有微软的好，是因为微软申请了专利，Google 绕不过去。&lt;/p&gt;
&lt;p&gt;Automating String Processing in Spreadsheets Using Input-Output Examples
&lt;a href=&#34;https://www.microsoft.com/en-us/research/wp-content/uploads/2016/12/popl11-synthesis.pdf&#34;&gt;https://www.microsoft.com/en-us/research/wp-content/uploads/2016/12/popl11-synthesis.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;via
&lt;a href=&#34;https://danluu.com/ballmer/&#34;&gt;https://danluu.com/ballmer/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
