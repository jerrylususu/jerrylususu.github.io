<!DOCTYPE html>
<html lang="zh">
	<head>
	<meta charset="utf-8"/>
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<meta name="generator" content="Hugo 0.145.0">
	<title>软文检测器 - Nekonull&#39;s Garden</title>

	<meta name="description" content="以及如何（错误地）部署一个 LLM app">


	
	
	




<link rel="stylesheet" href="/css/ui.css">

	
	

	<script defer src="/js/dark-mode.js"></script>
	<link disabled id="dark-mode-theme" rel="stylesheet" href="/css/dark.css">
	<link  rel="stylesheet" href="/css/dropdown.css">

	<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-3973436778637330"
	crossorigin="anonymous"></script>

	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Fira+Mono">
	
				
	
      <script async src="https://www.googletagmanager.com/gtag/js?id=G-R01JLDY2KE"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-R01JLDY2KE');
        }
      </script>
</head>

<body>
<header class="container no-print">
	<div class="u-header">
		<nav class="bar">
	<ul><li>
			<a href="/">
				<img class="icon-text" src="/img/prev.svg"/>
			</a>
		</li><li><img class="icon-text" id="dark-mode-toggle" src="/img/moon-regular.svg" alt="Toggle Dark Mode"></a></li><li><a href="/til">TIL</a></li><li><a href="/about">关于</a></li><li><a href="/archive">存档</a></li><li><a href="/tags">标签</a></li><li><a href="/share">短文</a></li><li><a href="/posts">长文</a></li><li><a href="/project">项目</a></li>
<li id="language-switch" class="dropdown">
<a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false"><img class="icon-text" src="/img/translate.svg" alt="Select Language"/></a>
<ul class="dropdown-menu">
  
  
  
      
      
          
      
          
            
            
                <li><a href="https://nekonull.me/posts/is-it-soft-ad/" class="active langselect">简体中文</a></li>
            
          
      
  
</ul>






	</ul>
</nav>

	</div>
</header>
<main class="container">

<article>
	<header><hgroup id="brand">
	<h1>软文检测器</h1>
	<h5>
		
		<time datetime="2025-09-21 22:58:00 &#43;0800 &#43;0800">2025/09/21 22:58</time>
		<span class="no-print">
			|
				
				<a href="/tags/LLM">LLM</a>
				<span>
	</h5>
	
</hgroup>
<hr class="sep" />
</header>
	<p>读某些新闻网站或公众号的时候，常常会越读越有种隐约的广告感（&ldquo;这是广告吗？&quot;），直到读到文末才彻底验证（”这果然就是广告吧！“）。之前买了 GLM 20 元一个月的 coding 套餐（agent 用的是 claude code），那就顺手 vibe coding 一个小工具来识别吧。于是就有了”软文检测器“。</p>
<ul>
<li>在线访问: <a href="https://nekonull.me/is-it-soft-ad/">https://nekonull.me/is-it-soft-ad/</a></li>
<li>Github: <a href="https://github.com/jerrylususu/is-it-soft-ad">https://github.com/jerrylususu/is-it-soft-ad</a></li>
</ul>
<p>这个小 app 本身并没有什么特别值得说道的，毕竟只是顺手做的玩具，代码我也没有仔细审查，看起来似乎没啥问题而且也能跑就提交了。作为一个 llm wraper，基本上就是一个 fastapi 后端加一个很简单的 vanilla js 前端（甚至没有用框架），前端提交内容后扔给 llm，然后用 sse 推送响应，来避免前端长时间卡住不出结果。模型上用的是 gemini-flash-2.0，是我能找到的速度-质量-价格最佳平衡点，而且 gemini flash 本身就极其便宜了，极大缓解了我的 llm 用量焦虑。prompt 是 deepseek r1 写的，在 repo 的 json 文件里可以找到，生成后稍微人工调整了一下但没有大改。</p>
<p>有人可能会好奇，在 llm 生成响应的过程中，返回的 json 并不完整，前端是如何保证展示有效内容的呢。实际上这里参考了苹果 wwdc 25 的 <a href="https://wwdcnotes.com/documentation/wwdcnotes/wwdc25-286-meet-the-foundation-models-framework/#Snapshot-streaming">Snapshot streaming</a>  的做法，虽然 llm 返回的响应并不是完整的 json，但是有库可以尝试从不完整的 json 解析出有效的 json，我用的是 <a href="https://github.com/promplate/partial-json-parser">promplate/partial-json-parser</a>（原理大概是自定义一个更宽松的parser） ，在后端包一层，就可以确保前端每次 sse 拿到的 event 都是有效 json了（虽然可能部分字段不完整）。</p>
<p>自己稍微玩了玩，确认没啥问题了之后，就开始想怎么部署到公网上。vibe coding 极大简化了写代码（如果你不在乎这个项目的长期可维护性，只是想创建个能用的 mvp）；但是 vibe deploying 并没有成熟度能与之相当的工具链。不少人用 supabase，但是对我这样一个简单的 llm wrapper 来说未免有点太重了，部署在别人的平台上也有点感觉不太安心（担心哪天超过 free tier 被收钱，虽然概率极其之小）。自己部署的话，又有不少前期工作需要做，例如子域名、网关、https、daemon化等等。我之前大部分分享出去的小工具都是纯前端的，扔在 github pages 上就完事了，最多写个 yaml 配置下 ci，带后端的对外工具实际上这还是第一个。最后分享欲还是战胜了嫌麻烦的心态，决定后端扔在一个不怎么用的云服务器上（服务器本身的开销可以忽略不计），前端还是放 github pages。最后满打满算下来，这个项目写代码&amp;自测的时间，可能还没有准备部署环境&amp;实际部署的时间长。</p>
<p>关于 llm 的应用，其实还涉及到一个控制用量（aka 限额）的问题。虽然我倒是不太介意在这种小玩具上每个月花点小钱（一瓶无糖可乐的价格都够一个便宜 llm 模型用很久了），但是我不想失去控制我能花多少钱的能力。一开始想的是在后端自己写个限额，或者直接用现成的库，但感觉涉及到钱的话还是有些危险，于是搁置了。手写限额的话，还需要考虑状态存储，就又多引入了一层复杂性（例如 cloudflare worker kv 每天只有 1k 写额度，上 durable object 或者 d1 也太重了。）顺带吐槽下，国内好几家主流的 LLM 服务提供商，都没有 by api key 限制使用额度的能力，但是在国外（如 openrouter）或各类中转商，这是基础的不能再基础的能力了。最后找了个支持 api key 限额的中转服务商，才算放下心来，知道再怎么刷也不会把我的账户刷爆了。</p>
<p>最后是可观测性，出于一些小小的私心，还是希望看看有多少人用过。为此前端加了 google analytics，后端用 opentelemetry 上报 trace 到 uptrace （他们家免费额度很宽裕）。最后实际上因为没怎么做推广（只在两个论坛上发了简单介绍），当然也没啥人用，最后大概是一天10个请求的样子，我自己的请求可能就占了一半。</p>
<p>一些未来的可能优化点（如果我还有兴趣在这个项目上继续投入的话）：</p>
<ul>
<li>把单纯的是否软文改成一个连续的光谱，或者至少多增加几个层级</li>
<li>现在对软文敏感度太高了，正常内容可能也会被认为是软文，看看怎么改 prompt 优化下</li>
<li>调查为啥得加 await asyncio.sleep 才能让 sse 正常工作</li>
<li>深入分析，补充到文内的引用</li>
<li>对文章按照”推广程度“染色（说实话我不太确定技术上这怎么实现）</li>
</ul>
<p>另外还有一些想法：</p>
<ul>
<li>可能对高级用户 bring your own key 的做法会更好，还能省去我部署后端的开发&amp;运营复杂度；可能可以考虑把这个项目转换成一个支持自己提供 llm 接入和使用现成后端的模式？
<ul>
<li>进一步地，是否存在这样的生态系统，提供了统一的 llm 接口（例如一个 js/ts sdk），用户只需要装一个插件，在插件内配置一次，各个网站就可以直接使用用户已配置的 llm 服务？（似乎 chrome 有个内置的小模型，但是很多事情小模型还是不太够用）</li>
</ul>
</li>
<li>之前试过扣子空间，虽然搭建简单原型没问题，但是分享起来异常困难（例如有审核流程）；如果存在更简单的分享&amp;创作方式，可能这类小但是有意义的 llm wrapper 会更广泛？</li>
<li>与其分享一个又一个 llm wrapper ，不如分享结构化 prompt 和支持运行结构化 prompt 的工具？（例如另一个之前的小创作 <a href="https://pages.nekonull.me/llm-prompt-templater.html">llm-prompt-templater</a> ）</li>
</ul>
<p>这篇文章有点混乱，想到哪里就写到哪里了，只是我个人的 brain dump。如果你能读到这里，那真是辛苦了。</p>

</article>
<nav class="no-print post-nav">

	<a class="prev-post" href="https://nekonull.me/posts/tmd-10-years/">
		<img class="icon-text" src="/img/prev.svg"/>和颞下颌关节紊乱共存的十年</a>


</nav>


<section id="related">
  <h4>另请参阅</h4>
  <ul>
    
  	<li><a href="/posts/llm_x_bookmark/">LLM x 书签收藏：摘要 &amp; 全文索引</a></li>
  	
  	<li><a href="/share/my-ai-prompts/">我的 AI Prompt</a></li>
  	
  	<li><a href="/share/try-chatglm/">用 mitmproxy 让 ChatGLM 适配 OpenAI 接口</a></li>
  	
  </ul>
</section>




	
  <script
    src="https://giscus.app/client.js"
    data-repo="jerrylususu/jerrylususu.github.io"
    data-repo-id="MDEwOlJlcG9zaXRvcnkxNDM1MTk3NDY="
    data-category="Blog Comments"
    data-category-id="DIC_kwDOCI3wAs4CZNcG"
    data-mapping="pathname"
    data-reactions-enabled="1"
    data-input-position="top"
    data-theme="preferred-color-scheme"
    data-lang="zh-CN"
    data-loading="lazy"
    data-strict="1"
    
    data-theme="preferred-color-scheme"
    crossorigin="anonymous"
    async
  ></script>

			<hr class="sep" />
		</main>
		<footer class="container no-print">
			<div class="u-footer">
				

<a href="https://github.com/jerrylususu/"><img class="icon-social" src="/img/github.svg" alt="Github"/></a>







				<p>
					
					使用的主题: <a href="https://github.com/yursan9/manis-hugo-theme">Manis</a><br>
					
					
					CC-BY-SA-4.0
					
					
				</p>
				
				<a href="#brand">
					<img class="icon-text" src="/img/toup.svg" alt="To Up"/>
					<span>回到顶部</span>
				</a>
				
			</div>
		</footer>
		
	</body>
</html>

